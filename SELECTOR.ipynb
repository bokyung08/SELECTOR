{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a90634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# data 준비\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 설정 ---------- #\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1   # 빠른 테스트용, 실제는 더 크게\n",
    "GAMMA = 0.9\n",
    "LR = 1e-3\n",
    "MEMORY_SIZE = 1000\n",
    "EPSILON = 0.1\n",
    "\n",
    "# GPU 사용 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- 데이터셋 로드 ---------- #\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 예시로 CIFAR10 사용, 다른 데이터셋에 맞게 변경 가능\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a8f9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 준비\n",
    "# ---------- 모델 후보 ---------- #\n",
    "def prepare_image_model(name):\n",
    "    if name == \"resnet18\":\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    elif name == \"googlenet\":\n",
    "        model = models.googlenet(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    elif name == \"mobilenet\":\n",
    "        model = models.mobilenet_v2(pretrained=True)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 10)\n",
    "    return model\n",
    "\n",
    "def prepare_text_model(name):\n",
    "    if name == \"bert\":\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    return model, tokenizer\n",
    "\n",
    "model_list_img = [\"resnet18\", \"googlenet\", \"mobilenet\"]\n",
    "model_list_txt = [\"bert\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f9daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#상태 벡터\n",
    "# ---------- 상태 벡터 ---------- #\n",
    "def extract_state(dataset):\n",
    "    num_classes = len(set(dataset.targets))\n",
    "    num_samples = len(dataset)\n",
    "    return torch.tensor([num_samples / 10000, num_classes / 10]).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa27e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQN model\n",
    "# ---------- DQN 네트워크 ---------- #\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33a6be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, is_image_model=True, tokenizer=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            if is_image_model:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "            else:\n",
    "                texts, labels = data\n",
    "                inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(**inputs).last_hidden_state[:, 0, :]\n",
    "\n",
    "            # 간단한 accuracy 계산용 임시 로직\n",
    "            pred = torch.argmax(outputs, dim=1) if is_image_model else torch.zeros_like(labels)  # ← 텍스트 모델용 정확도 임시 처리\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total if total > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04cb2dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 모델 평가\u001b[39;00m\n\u001b[0;32m     45\u001b[0m img_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(img_model, trainloader, is_image_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 46\u001b[0m txt_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_image_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m reward \u001b[38;5;241m=\u001b[39m (img_accuracy \u001b[38;5;241m+\u001b[39m txt_accuracy) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# 두 모델 성능 평균\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 상태 업데이트 및 메모리 저장\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[36], line 15\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, dataloader, is_image_model, tokenizer)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     texts, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m---> 15\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2602\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2600\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2601\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2602\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2604\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2660\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2657\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 2660\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2661\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2662\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2663\u001b[0m     )\n\u001b[0;32m   2665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   2666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2667\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2668\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2669\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "#강화학습\n",
    "# ---------- 강화학습 루프 ---------- #\n",
    "state_dim = 2  # 상태 벡터의 크기\n",
    "action_dim_img = len(model_list_img)\n",
    "action_dim_txt = len(model_list_txt)\n",
    "\n",
    "dqn_img = DQN(state_dim, action_dim_img).to(device)\n",
    "dqn_txt = DQN(state_dim, action_dim_txt).to(device)\n",
    "\n",
    "optimizer_img = optim.Adam(dqn_img.parameters(), lr=LR)\n",
    "optimizer_txt = optim.Adam(dqn_txt.parameters(), lr=LR)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "memory_img = deque(maxlen=MEMORY_SIZE)\n",
    "memory_txt = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# DQN 훈련 루프\n",
    "for episode in range(10):  # 에피소드 반복\n",
    "    state = extract_state(trainset)  # 상태 벡터\n",
    "    \n",
    "    # 이미지 모델 선택\n",
    "    if random.random() < EPSILON:\n",
    "        action_img = random.randint(0, action_dim_img - 1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = dqn_img(state)\n",
    "            action_img = torch.argmax(q_values).item()\n",
    "\n",
    "    # 텍스트 모델 선택\n",
    "    if random.random() < EPSILON:\n",
    "        action_txt = random.randint(0, action_dim_txt - 1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = dqn_txt(state)\n",
    "            action_txt = torch.argmax(q_values).item()\n",
    "\n",
    "    # 모델 준비\n",
    "    img_model = prepare_image_model(model_list_img[action_img])\n",
    "    txt_model, tokenizer = prepare_text_model(model_list_txt[action_txt])\n",
    "\n",
    "    \n",
    "\n",
    "    # 모델 평가\n",
    "    img_accuracy = evaluate_model(img_model, trainloader, is_image_model=True)\n",
    "    txt_accuracy = evaluate_model(txt_model, trainloader, is_image_model=False, tokenizer=tokenizer)\n",
    "\n",
    "    reward = (img_accuracy + txt_accuracy) / 2  # 두 모델 성능 평균\n",
    "\n",
    "    # 상태 업데이트 및 메모리 저장\n",
    "    next_state = state.clone()\n",
    "\n",
    "    memory_img.append((state, action_img, reward, next_state))\n",
    "    memory_txt.append((state, action_txt, reward, next_state))\n",
    "\n",
    "    # 이미지 모델 경험 리플레이 학습\n",
    "    if len(memory_img) >= BATCH_SIZE:\n",
    "        batch = random.sample(memory_img, BATCH_SIZE)\n",
    "        batch_states, batch_actions, batch_rewards, batch_next_states = zip(*batch)\n",
    "\n",
    "        batch_states = torch.stack(batch_states).to(device)\n",
    "        batch_actions = torch.tensor(batch_actions).to(device)\n",
    "        batch_rewards = torch.tensor(batch_rewards).to(device)\n",
    "\n",
    "        q_values = dqn_img(batch_states).gather(1, batch_actions.unsqueeze(1)).squeeze()\n",
    "        loss = loss_fn(q_values, batch_rewards)\n",
    "\n",
    "        optimizer_img.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_img.step()\n",
    "\n",
    "    # 텍스트 모델 경험 리플레이 학습\n",
    "    if len(memory_txt) >= BATCH_SIZE:\n",
    "        batch = random.sample(memory_txt, BATCH_SIZE)\n",
    "        batch_states, batch_actions, batch_rewards, batch_next_states = zip(*batch)\n",
    "\n",
    "\n",
    "        batch_states = torch.stack(batch_states).to(device)\n",
    "        batch_actions = torch.tensor(batch_actions).to(device)\n",
    "        batch_rewards = torch.tensor(batch_rewards).to(device)\n",
    "\n",
    "        q_values = dqn_txt(batch_states).gather(1, batch_actions.unsqueeze(1)).squeeze()\n",
    "        loss = loss_fn(q_values, batch_rewards)\n",
    "\n",
    "        optimizer_txt.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_txt.step()\n",
    "\n",
    "    print(f\"[에피소드 {episode}] 이미지 정확도: {img_accuracy:.4f}, 텍스트 정확도: {txt_accuracy:.4f}, 보상: {reward:.4f}\")\n",
    "\n",
    "print(\"훈련 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06dc0302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[에피소드 0] 이미지 정확도: 0.0634, 텍스트 정확도(더미): 0.5000, 보상: 0.2817\n",
      "[에피소드 1] 이미지 정확도: 0.0855, 텍스트 정확도(더미): 0.5000, 보상: 0.2927\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 117\u001b[0m\n\u001b[0;32m    115\u001b[0m img_model \u001b[38;5;241m=\u001b[39m prepare_image_model(model_list_img[action_img])\n\u001b[0;32m    116\u001b[0m txt_model, tokenizer \u001b[38;5;241m=\u001b[39m prepare_text_model(model_list_txt[action_txt])\n\u001b[1;32m--> 117\u001b[0m img_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_image_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m txt_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(txt_model, trainloader, is_image_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m    120\u001b[0m reward \u001b[38;5;241m=\u001b[39m (img_accuracy \u001b[38;5;241m+\u001b[39m txt_accuracy) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "Cell \u001b[1;32mIn[40], line 77\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, dataloader, is_image_model, tokenizer)\u001b[0m\n\u001b[0;32m     75\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     76\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 77\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m pred \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     79\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m--> 274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[0;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 설정\n",
    "BATCH_SIZE = 8  # 작은 배치로 테스트\n",
    "EPOCHS = 1\n",
    "GAMMA = 0.9\n",
    "LR = 1e-3\n",
    "MEMORY_SIZE = 1000\n",
    "EPSILON = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 데이터셋 로드 (CIFAR10)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 이미지 모델 후보\n",
    "def prepare_image_model(name):\n",
    "    if name == \"resnet18\":\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    elif name == \"googlenet\":\n",
    "        model = models.googlenet(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    elif name == \"mobilenet\":\n",
    "        model = models.mobilenet_v2(pretrained=True)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 10)\n",
    "    return model.to(device).eval()\n",
    "\n",
    "# 텍스트 모델 후보 (샘플용 토크나이저만 반환)\n",
    "def prepare_text_model(name):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').to(device).eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "# 상태 벡터 추출\n",
    "def extract_state(dataset):\n",
    "    num_classes = len(set(dataset.targets))\n",
    "    num_samples = len(dataset)\n",
    "    return torch.tensor([num_samples / 10000, num_classes / 10]).float().to(device)\n",
    "\n",
    "# DQN 정의\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 평가 함수 (텍스트 모델은 더미 정확도 반환)\n",
    "def evaluate_model(model, dataloader, is_image_model=True, tokenizer=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            if is_image_model:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            else:\n",
    "                # 텍스트 모델 더미 정확도\n",
    "                return 0.5  # 임의 값\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# DQN 학습 설정\n",
    "model_list_img = [\"resnet18\", \"googlenet\", \"mobilenet\"]\n",
    "model_list_txt = [\"bert\"]\n",
    "state_dim = 2\n",
    "action_dim_img = len(model_list_img)\n",
    "action_dim_txt = len(model_list_txt)\n",
    "\n",
    "dqn_img = DQN(state_dim, action_dim_img).to(device)\n",
    "dqn_txt = DQN(state_dim, action_dim_txt).to(device)\n",
    "optimizer_img = optim.Adam(dqn_img.parameters(), lr=LR)\n",
    "optimizer_txt = optim.Adam(dqn_txt.parameters(), lr=LR)\n",
    "loss_fn = nn.MSELoss()\n",
    "memory_img = deque(maxlen=MEMORY_SIZE)\n",
    "memory_txt = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# 훈련 루프\n",
    "for episode in range(3):\n",
    "    state = extract_state(trainset)\n",
    "\n",
    "    # 모델 선택 (Epsilon-greedy)\n",
    "    with torch.no_grad():\n",
    "        q_values_img = dqn_img(state)\n",
    "    action_img = torch.argmax(q_values_img).item() if random.random() > EPSILON else random.randint(0, action_dim_img - 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_values_txt = dqn_txt(state)\n",
    "    action_txt = torch.argmax(q_values_txt).item() if random.random() > EPSILON else random.randint(0, action_dim_txt - 1)\n",
    "\n",
    "    # 모델 로딩 및 평가\n",
    "    img_model = prepare_image_model(model_list_img[action_img])\n",
    "    txt_model, tokenizer = prepare_text_model(model_list_txt[action_txt])\n",
    "    img_accuracy = evaluate_model(img_model, trainloader, is_image_model=True)\n",
    "    txt_accuracy = evaluate_model(txt_model, trainloader, is_image_model=False, tokenizer=tokenizer)\n",
    "\n",
    "    reward = (img_accuracy + txt_accuracy) / 2\n",
    "    next_state = state.clone()\n",
    "    memory_img.append((state, action_img, reward, next_state))\n",
    "    memory_txt.append((state, action_txt, reward, next_state))\n",
    "\n",
    "    # 학습 (이미지 DQN)\n",
    "    if len(memory_img) >= BATCH_SIZE:\n",
    "        batch = random.sample(memory_img, BATCH_SIZE)\n",
    "        batch_states, batch_actions, batch_rewards, _ = zip(*batch)\n",
    "        batch_states = torch.stack(batch_states).to(device)\n",
    "        batch_actions = torch.tensor(batch_actions).to(device)\n",
    "        batch_rewards = torch.tensor(batch_rewards).to(device)\n",
    "        q_values = dqn_img(batch_states).gather(1, batch_actions.unsqueeze(1)).squeeze()\n",
    "        loss = loss_fn(q_values, batch_rewards)\n",
    "        optimizer_img.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_img.step()\n",
    "\n",
    "    # 학습 (텍스트 DQN)\n",
    "    if len(memory_txt) >= BATCH_SIZE:\n",
    "        batch = random.sample(memory_txt, BATCH_SIZE)\n",
    "        batch_states, batch_actions, batch_rewards, _ = zip(*batch)\n",
    "        batch_states = torch.stack(batch_states).to(device)\n",
    "        batch_actions = torch.tensor(batch_actions).to(device)\n",
    "        batch_rewards = torch.tensor(batch_rewards).to(device)\n",
    "        q_values = dqn_txt(batch_states).gather(1, batch_actions.unsqueeze(1)).squeeze()\n",
    "        loss = loss_fn(q_values, batch_rewards)\n",
    "        optimizer_txt.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_txt.step()\n",
    "\n",
    "    print(f\"[에피소드 {episode}] 이미지 정확도: {img_accuracy:.4f}, 텍스트 정확도(더미): {txt_accuracy:.4f}, 보상: {reward:.4f}\")\n",
    "\n",
    "print(\"훈련 완료\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f219eaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to C:\\Users\\bokyung/.cache\\torch\\hub\\checkpoints\\densenet121-a639ec97.pth\n",
      "100%|██████████| 30.8M/30.8M [00:02<00:00, 11.7MB/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 0] 이미지 정확도: 0.0010, 텍스트 정확도: 0.2513, 보상: 0.1262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 1] 이미지 정확도: 0.0036, 텍스트 정확도: 0.3053, 보상: 0.1545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 2] 이미지 정확도: 0.0047, 텍스트 정확도: 0.2534, 보상: 0.1290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 3] 이미지 정확도: 0.0001, 텍스트 정확도: 0.2469, 보상: 0.1235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 4] 이미지 정확도: 0.0083, 텍스트 정확도: 0.2404, 보상: 0.1244\n",
      "✅ 훈련 완료\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import Caltech101\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision.datasets import CIFAR10\n",
    "# ---------- 설정 ---------- #\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1  # 빠른 테스트용\n",
    "GAMMA = 0.95\n",
    "LR = 1e-4\n",
    "MEMORY_SIZE = 1000\n",
    "EPSILON = 0.2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- 이미지 데이터셋 (Caltech101) ---------- #\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "image_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "image_loader = DataLoader(image_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ---------- 텍스트 데이터셋 (AG News) ---------- #\n",
    "ag_dataset = load_dataset(\"ag_news\")\n",
    "tokenizer_text = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.texts = ag_dataset[split][\"text\"]\n",
    "        self.labels = ag_dataset[split][\"label\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "text_dataset = AGNewsDataset(\"train\")\n",
    "text_loader = DataLoader(text_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ---------- 상태 벡터 ---------- #\n",
    "def extract_state(dataset, data_type=\"image\"):\n",
    "    if data_type == \"image\":\n",
    "        num_classes = len(dataset.classes)\n",
    "        num_samples = len(dataset)\n",
    "    else:\n",
    "        num_classes = 4  # AG News는 고정\n",
    "        num_samples = len(dataset)\n",
    "    return torch.tensor([num_samples / 10000, num_classes / 10]).float().to(device)\n",
    "\n",
    "# ---------- 모델 로딩 ---------- #\n",
    "def load_text_model(name=\"bert-base-uncased\"):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(name, num_labels=4).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_image_model(name):\n",
    "    if name == \"resnet18\":\n",
    "        model = torchvision.models.resnet18(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 102)\n",
    "    elif name == \"mobilenet\":\n",
    "        model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 102)\n",
    "    elif name == \"densenet\":\n",
    "        model = torchvision.models.densenet121(pretrained=True)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, 102)\n",
    "    return model.to(device)\n",
    "\n",
    "# ---------- 평가 함수 ---------- #\n",
    "def evaluate_image_model(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(y.cpu().numpy())\n",
    "    return accuracy_score(labels, preds)\n",
    "\n",
    "def evaluate_text_model(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in dataloader:\n",
    "            inputs = tokenizer(list(texts), return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(**inputs)\n",
    "            pred = torch.argmax(output.logits, dim=1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    return correct / total\n",
    "\n",
    "# ---------- DQN 정의 ---------- #\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "dqn_img = DQN(state_dim=2, action_dim=3).to(device)\n",
    "dqn_txt = DQN(state_dim=2, action_dim=1).to(device)\n",
    "optimizer_img = optim.Adam(dqn_img.parameters(), lr=LR)\n",
    "optimizer_txt = optim.Adam(dqn_txt.parameters(), lr=LR)\n",
    "loss_fn = nn.MSELoss()\n",
    "memory_img, memory_txt = deque(maxlen=MEMORY_SIZE), deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# ---------- 학습 루프 ---------- #\n",
    "for episode in range(5):\n",
    "    state_img = extract_state(image_dataset, \"image\")\n",
    "    state_txt = extract_state(text_dataset, \"text\")\n",
    "\n",
    "    # 이미지 모델 선택\n",
    "    action_img = random.randint(0, 2) if random.random() < EPSILON else torch.argmax(dqn_img(state_img)).item()\n",
    "    # 텍스트 모델은 1개 고정\n",
    "    action_txt = 0\n",
    "\n",
    "    img_model = load_image_model([\"resnet18\", \"mobilenet\", \"densenet\"][action_img])\n",
    "    txt_model, tokenizer = load_text_model(\"bert-base-uncased\")\n",
    "\n",
    "    # 성능 평가\n",
    "    img_acc = evaluate_image_model(img_model, image_loader)\n",
    "    txt_acc = evaluate_text_model(txt_model, text_loader, tokenizer)\n",
    "    reward = (img_acc + txt_acc) / 2\n",
    "\n",
    "    memory_img.append((state_img, action_img, reward, state_img.clone()))\n",
    "    memory_txt.append((state_txt, action_txt, reward, state_txt.clone()))\n",
    "\n",
    "    # 경험 리플레이 - 이미지\n",
    "    if len(memory_img) >= BATCH_SIZE:\n",
    "        batch = random.sample(memory_img, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.tensor(actions).unsqueeze(1).to(device)\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "\n",
    "        q_vals = dqn_img(states).gather(1, actions).squeeze()\n",
    "        loss = loss_fn(q_vals, rewards)\n",
    "        optimizer_img.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_img.step()\n",
    "\n",
    "    # 경험 리플레이 - 텍스트\n",
    "    if len(memory_txt) >= BATCH_SIZE:\n",
    "        batch = random.sample(memory_txt, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.tensor(actions).unsqueeze(1).to(device)\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "\n",
    "        q_vals = dqn_txt(states).gather(1, actions).squeeze()\n",
    "        loss = loss_fn(q_vals, rewards)\n",
    "        optimizer_txt.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_txt.step()\n",
    "\n",
    "    print(f\"[EP {episode}] 이미지 정확도: {img_acc:.4f}, 텍스트 정확도: {txt_acc:.4f}, 보상: {reward:.4f}\")\n",
    "print(\"✅ 훈련 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6ecd56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Selected=resnet18, Accuracy=0.5843\n",
      "Episode 2: Selected=resnet18, Accuracy=0.4973\n",
      "Episode 3: Selected=densenet, Accuracy=0.6301\n",
      "Episode 4: Selected=densenet, Accuracy=0.6242\n",
      "Episode 5: Selected=densenet, Accuracy=0.6016\n",
      "Episode 6: Selected=mobilenet, Accuracy=0.5062\n",
      "Episode 7: Selected=densenet, Accuracy=0.5819\n",
      "Episode 8: Selected=densenet, Accuracy=0.5933\n",
      "Episode 9: Selected=densenet, Accuracy=0.6084\n",
      "Episode 10: Selected=densenet, Accuracy=0.6667\n",
      "모델 선택기 학습 완료.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, DistilBertForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# -------------------------------\n",
    "# 1. 환경 설정\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. 모델 후보\n",
    "# -------------------------------\n",
    "\n",
    "IMAGE_MODELS = {\n",
    "    \"resnet18\": torchvision.models.resnet18,\n",
    "    \"mobilenet\": torchvision.models.mobilenet_v2,\n",
    "    \"densenet\": torchvision.models.densenet121,\n",
    "}\n",
    "\n",
    "TEXT_MODELS = {\n",
    "    \"bert-base-uncased\": BertForSequenceClassification,\n",
    "    \"distilbert-base-uncased\": DistilBertForSequenceClassification,\n",
    "    \"roberta-base\": RobertaForSequenceClassification,\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. DQN (선택기)\n",
    "# -------------------------------\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. 간단한 이미지 데이터셋 로딩 (CIFAR10)\n",
    "# -------------------------------\n",
    "\n",
    "def get_image_dataset():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    return train_set, test_set\n",
    "\n",
    "# -------------------------------\n",
    "# 5. 데이터 특성 추출 함수\n",
    "# -------------------------------\n",
    "\n",
    "def extract_image_state(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    images, labels = next(iter(loader))\n",
    "    mean = images.mean().item()\n",
    "    std = images.std().item()\n",
    "    num_classes = len(set(labels.numpy()))\n",
    "    return torch.tensor([mean, std, num_classes], dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. 모델 학습 및 평가 함수\n",
    "# -------------------------------\n",
    "\n",
    "def train_and_evaluate_image_model(model_fn, train_set, test_set, epochs=1):\n",
    "    model = model_fn(pretrained=False, num_classes=10).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 평가\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# -------------------------------\n",
    "# 7. 메인 루프: 모델 선택기 훈련\n",
    "# -------------------------------\n",
    "\n",
    "def train_model_selector():\n",
    "    train_set, test_set = get_image_dataset()\n",
    "\n",
    "    dqn = DQN(input_dim=3, output_dim=len(IMAGE_MODELS)).to(device)\n",
    "    optimizer = optim.Adam(dqn.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for episode in range(10):\n",
    "        state = extract_image_state(train_set).to(device)\n",
    "\n",
    "        # DQN이 선택한 모델\n",
    "        q_values = dqn(state)\n",
    "        action = torch.argmax(q_values).item()\n",
    "        selected_model_key = list(IMAGE_MODELS.keys())[action]\n",
    "        model_fn = IMAGE_MODELS[selected_model_key]\n",
    "\n",
    "        # 선택된 모델로 학습 및 평가\n",
    "        reward = train_and_evaluate_image_model(model_fn, train_set, test_set, epochs=1)\n",
    "\n",
    "        # Q 업데이트\n",
    "        target = q_values.clone().detach()\n",
    "        target[action] = reward\n",
    "\n",
    "        # 학습\n",
    "        output = dqn(state)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Episode {episode + 1}: Selected={selected_model_key}, Accuracy={reward:.4f}\")\n",
    "\n",
    "    print(\"모델 선택기 학습 완료.\")\n",
    "\n",
    "train_model_selector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9bdb3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\bokyung\\AppData\\Local\\Temp\\ipykernel_17288\\4287074650.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Selected=bert, Accuracy=0.9333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2: Selected=bert, Accuracy=0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3: Selected=roberta, Accuracy=0.9033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4: Selected=distilbert, Accuracy=0.9533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5: Selected=distilbert, Accuracy=0.9400\n",
      "텍스트 모델 선택기 학습 완료.\n"
     ]
    }
   ],
   "source": [
    "# Adaptive Model Selector using DQN (Text Version)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, \\\n",
    "                         DistilBertTokenizer, DistilBertForSequenceClassification, \\\n",
    "                         RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "# AG News 데이터 불러오기 (train만 사용)\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# 텍스트 및 라벨 추출\n",
    "raw_texts = dataset['train']['text']\n",
    "raw_labels = dataset['train']['label']  # 0~3: World, Sports, Business, Sci/Tech\n",
    "\n",
    "# 이진 분류용으로 라벨 변경 (예: World/Sports vs Business/SciTech)\n",
    "texts = raw_texts[:1000]  # 빠른 테스트를 위해 1000개만 사용\n",
    "labels = [1 if label in [0, 1] else 0 for label in raw_labels[:1000]]\n",
    "\n",
    "\n",
    "# 모델 후보\n",
    "TEXT_MODELS = {\n",
    "    \"bert\": (BertTokenizer.from_pretrained(\"bert-base-uncased\"), BertForSequenceClassification),\n",
    "    \"distilbert\": (DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\"), DistilBertForSequenceClassification),\n",
    "    \"roberta\": (RobertaTokenizer.from_pretrained(\"roberta-base\"), RobertaForSequenceClassification),\n",
    "}\n",
    "\n",
    "# DQN 정의\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# 커스텀 Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, torch.tensor(self.labels[idx])\n",
    "\n",
    "# 특성 추출\n",
    "def extract_text_state(texts):\n",
    "    lengths = [len(t.split()) for t in texts]\n",
    "    mean_len = np.mean(lengths)\n",
    "    std_len = np.std(lengths)\n",
    "    vocab_size = len(set(word for text in texts for word in text.lower().split()))\n",
    "    return torch.tensor([mean_len, std_len, vocab_size], dtype=torch.float32)\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "def train_and_evaluate_text_model(tokenizer, model_cls, train_texts, train_labels, test_texts, test_labels, epochs=1):\n",
    "    model = model_cls.from_pretrained(tokenizer.name_or_path, num_labels=2).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    train_dataset = TextDataset(train_encodings, train_labels)\n",
    "    test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# DQN 훈련 루프\n",
    "def train_text_model_selector():\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.3)\n",
    "\n",
    "    dqn = DQN(input_dim=3, output_dim=len(TEXT_MODELS)).to(device)\n",
    "    optimizer = optim.Adam(dqn.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for episode in range(5):\n",
    "        state = extract_text_state(train_texts).to(device)\n",
    "\n",
    "        q_values = dqn(state)\n",
    "        action = torch.argmax(q_values).item()\n",
    "        selected_key = list(TEXT_MODELS.keys())[action]\n",
    "        tokenizer, model_cls = TEXT_MODELS[selected_key]\n",
    "\n",
    "        reward = train_and_evaluate_text_model(tokenizer, model_cls, train_texts, train_labels, test_texts, test_labels)\n",
    "\n",
    "        target = q_values.clone().detach()\n",
    "        target[action] = reward\n",
    "\n",
    "        output = dqn(state)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Episode {episode+1}: Selected={selected_key}, Accuracy={reward:.4f}\")\n",
    "\n",
    "    print(\"텍스트 모델 선택기 학습 완료.\")\n",
    "\n",
    "train_text_model_selector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a91030e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.64G/2.64G [09:54<00:00, 4.44MB/s]  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 이미지 상태 벡터 생성 함수 호출\u001b[39;00m\n\u001b[0;32m     18\u001b[0m first_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m image_state \u001b[38;5;241m=\u001b[39m \u001b[43mextract_image_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 모델 선택기 훈련\u001b[39;00m\n\u001b[0;32m     22\u001b[0m train_image_model_selector(train_loader, test_loader, image_state)\n",
      "Cell \u001b[1;32mIn[52], line 74\u001b[0m, in \u001b[0;36mextract_image_state\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_image_state\u001b[39m(dataset):\n\u001b[0;32m     73\u001b[0m     loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(loader))\n\u001b[0;32m     75\u001b[0m     mean \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     76\u001b[0m     std \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import STL10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# STL10 데이터셋 로드 및 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "stl_train = STL10(root='./data', split='train', download=True, transform=transform)\n",
    "stl_test = STL10(root='./data', split='test', download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(stl_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(stl_test, batch_size=64, shuffle=False)\n",
    "\n",
    "# 이미지 상태 벡터 생성 함수 호출\n",
    "first_batch = next(iter(train_loader))[0]\n",
    "image_state = extract_image_state(first_batch)\n",
    "\n",
    "# 모델 선택기 훈련\n",
    "train_image_model_selector(train_loader, test_loader, image_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf8b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Yelp 데이터셋 로딩 (샘플 제한 가능)\n",
    "dataset = load_dataset(\"yelp_polarity\")\n",
    "texts = dataset['train']['text'][:2000]\n",
    "labels = dataset['train']['label'][:2000]\n",
    "\n",
    "# 상태 벡터 추출\n",
    "state = extract_text_state(texts)\n",
    "\n",
    "# 텍스트 모델 선택기 훈련\n",
    "train_text_model_selector(texts, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5caaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bokyung\\anaconda3\\envs\\mygpuenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\bokyung/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:48<00:00, 11.5MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\bokyung\\AppData\\Local\\Temp\\ipykernel_17288\\3254784300.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Selected=distilbert, Accuracy=0.9433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2: Selected=distilbert, Accuracy=0.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3: Selected=distilbert, Accuracy=0.9467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, \\\n",
    "                         DistilBertTokenizer, DistilBertForSequenceClassification, \\\n",
    "                         RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# 환경 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 텍스트 모델 후보\n",
    "TEXT_MODELS = {\n",
    "    \"bert\": (BertTokenizer.from_pretrained(\"bert-base-uncased\"), BertForSequenceClassification),\n",
    "    \"distilbert\": (DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\"), DistilBertForSequenceClassification),\n",
    "    \"roberta\": (RobertaTokenizer.from_pretrained(\"roberta-base\"), RobertaForSequenceClassification),\n",
    "}\n",
    "\n",
    "# 이미지 모델 후보\n",
    "IMAGE_MODELS = {\n",
    "    \"resnet18\": models.resnet18(pretrained=True),\n",
    "    \"vgg16\": models.vgg16(pretrained=True),\n",
    "}\n",
    "\n",
    "# DQN 정의 (모델 선택기)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# AG News 데이터 불러오기 (train만 사용)\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# 텍스트 및 라벨 추출\n",
    "raw_texts = dataset['train']['text']\n",
    "raw_labels = dataset['train']['label']  # 0~3: World, Sports, Business, Sci/Tech\n",
    "\n",
    "# 이진 분류용으로 라벨 변경 (예: World/Sports vs Business/SciTech)\n",
    "texts = raw_texts[:1000]  # 빠른 테스트를 위해 1000개만 사용\n",
    "labels = [1 if label in [0, 1] else 0 for label in raw_labels[:1000]]\n",
    "\n",
    "# 이미지 데이터 예시 (단순화된 예시)\n",
    "image_paths = [\"image1.jpg\", \"image2.jpg\"]  # 실제 이미지 경로를 입력하세요\n",
    "image_labels = [0, 1]  # 예시 라벨\n",
    "\n",
    "# 텍스트 데이터셋 정의\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, torch.tensor(self.labels[idx])\n",
    "\n",
    "# 이미지 데이터셋 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(self.labels[idx])\n",
    "\n",
    "# 이미지 데이터에 대한 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 텍스트 모델 학습 및 평가\n",
    "def train_and_evaluate_text_model(tokenizer, model_cls, train_texts, train_labels, test_texts, test_labels, epochs=1):\n",
    "    model = model_cls.from_pretrained(tokenizer.name_or_path, num_labels=2).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    train_dataset = TextDataset(train_encodings, train_labels)\n",
    "    test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# 이미지 모델 학습 및 평가\n",
    "def train_and_evaluate_image_model(model, train_images, train_labels, test_images, test_labels, epochs=1):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_dataset = ImageDataset(train_images, train_labels, transform=transform)\n",
    "    test_dataset = ImageDataset(test_images, test_labels, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# 데이터 유형에 맞는 모델 선택\n",
    "def select_model(data_type, data, train_labels, test_labels):\n",
    "    if data_type == 'text':\n",
    "        # 텍스트 데이터에 적합한 모델을 선택\n",
    "        tokenizer, model_cls = TEXT_MODELS[\"bert\"]\n",
    "        accuracy = train_and_evaluate_text_model(tokenizer, model_cls, data, train_labels, test_labels)\n",
    "    elif data_type == 'image':\n",
    "        # 이미지 데이터에 적합한 모델을 선택\n",
    "        model = IMAGE_MODELS[\"resnet18\"]\n",
    "        accuracy = train_and_evaluate_image_model(model, data, train_labels, test_labels)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown data type\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# 텍스트 모델 선택기 학습\n",
    "def train_text_model_selector():\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.3)\n",
    "    \n",
    "    dqn = DQN(input_dim=3, output_dim=len(TEXT_MODELS)).to(device)\n",
    "    optimizer = optim.Adam(dqn.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for episode in range(5):\n",
    "        state = torch.tensor([1.0, 0.5, 10.0], dtype=torch.float32).to(device)  # 예시 상태\n",
    "        q_values = dqn(state)\n",
    "        action = torch.argmax(q_values).item()\n",
    "        selected_key = list(TEXT_MODELS.keys())[action]\n",
    "        tokenizer, model_cls = TEXT_MODELS[selected_key]\n",
    "\n",
    "        reward = train_and_evaluate_text_model(tokenizer, model_cls, train_texts, train_labels, test_texts, test_labels)\n",
    "\n",
    "        target = q_values.clone().detach()\n",
    "        target[action] = reward\n",
    "\n",
    "        output = dqn(state)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Episode {episode+1}: Selected={selected_key}, Accuracy={reward:.4f}\")\n",
    "\n",
    "    print(\"텍스트 모델 선택기 학습 완료.\")\n",
    "\n",
    "# 예시 실행\n",
    "train_text_model_selector()\n",
    "\n",
    "# 이미지 모델 선택기 사용 예시\n",
    "image_accuracy = select_model('image', image_paths, image_labels, image_labels)\n",
    "print(\"이미지 모델 정확도:\", image_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygpuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
